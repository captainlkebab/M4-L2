# ðŸ“š **Exercise: Building an SQLite3 Database from a Scraped CSV File**

## **Background**
You are building an app that scrapes news articles daily and uses an LLM to write a report summarizing the key events from the previous day. To keep track of the articles and reports, you need to create a database in SQLite3 to store the scraped articles, generated reports, and related metadata.

You have the scraped data stored as a CSV file with the following structure:

| Column Name | Description |
|------------|-------------|
| `url`        | URL of the article |
| `title`      | Title of the article |
| `label`      | Label associated with the article (e.g., "Analysis") |
| `theme`      | Theme of the article |
| `badge`      | Badge or tag for the article |
| `datetime`   | Date and time of publication |
| `author`     | Author of the article |
| `text`       | Full content of the article |

---

## **Objective**
Your task is to create an SQLite3 database in Python that includes three specific tables with well-defined keys and constraints. The database should handle daily scraping of articles, store the articles and reports, and ensure that no duplicate articles are stored.

---

## **Requirements**
You will create the following three tables:

### 1. **Articles Table**  
This table will store the details of the scraped articles.

| Column Name | Data Type | Description | Constraints |
|------------|-----------|-------------|------------|
| `article_id`   | INTEGER    | Unique identifier for each article | PRIMARY KEY, AUTOINCREMENT |
| `url`          | TEXT       | URL of the article                 | UNIQUE, NOT NULL          |
| `title`        | TEXT       | Title of the article               | NULLABLE                  |
| `label`        | TEXT       | Label for the article              | NULLABLE                  |
| `theme`        | TEXT       | Theme of the article               | NULLABLE                  |
| `badge`        | TEXT       | Badge for the article              | NULLABLE                  |
| `datetime`     | TEXT       | Date and time of publication       | NOT NULL                  |
| `author`       | TEXT       | Author of the article              | NULLABLE                  |
| `text`         | TEXT       | Full content of the article        | NOT NULL                  |

---

### 2. **Reports Table**  
This table will store the reports generated by the LLM.

| Column Name | Data Type | Description | Constraints |
|-------------|-----------|-------------|------------|
| `report_id`   | INTEGER    | Unique identifier for each report | PRIMARY KEY, AUTOINCREMENT |
| `report_date` | TEXT       | Date of the report                | UNIQUE, NOT NULL          |
| `content`     | TEXT       | Full text of the report            | NOT NULL                  |

---

### 3. **Article_Report_Link Table**  
This table will store the relationship between the articles and the reports (as one report can summarize multiple articles).

| Column Name | Data Type | Description | Constraints |
|-------------|-----------|-------------|------------|
| `id`         | INTEGER    | Unique identifier for each link   | PRIMARY KEY, AUTOINCREMENT |
| `article_id`  | INTEGER    | ID of the related article          | FOREIGN KEY REFERENCES `Articles(article_id)` ON DELETE CASCADE |
| `report_id`   | INTEGER    | ID of the related report           | FOREIGN KEY REFERENCES `Reports(report_id)` ON DELETE CASCADE |

---

## **Functional Requirements**
1. **Your code should**:
   - Create the three tables in SQLite3 using Python.
   - Ensure that existing articles (based on the `url`) are not replaced when new articles are scraped.
   - Allow linking of multiple articles to a single report.
   - Automatically delete links in `Article_Report_Link` if the associated article or report is deleted.

2. **The app should**:
   - Read the scraped data from a CSV file.
   - Convert `NaN` values to `None` to match SQLite's `NULL`.
   - Allow inserting new articles.
   - Check for duplicates based on `url` before inserting a new article.
   - Allow inserting a new report and linking it to existing articles.

3. **(Optional) Create some sample data**:
   - Add at least **5 sample articles** (based on the example data).
   - Add **1 report** summarizing 2â€“3 of the articles.

---

## **Input Example**
Example CSV file (`scraped_news.csv`):

```csv
url,title,label,theme,badge,datetime,author,text
https://www.dr.dk/nyheder/udland/syrer-i-danmark,Syrer i Danmark,,Magtskifte i Syrien,,2025-03-09T20:24:00+00:00,Silas Bay Nielsen,Syrer i Danmark content...
https://www.dr.dk/nyheder/udland/analysis-europe,,,,,2025-03-09T20:02:00+00:00,,Analysis content...
https://www.dr.dk/nyheder/udland/analysis-germany,,,,,2025-03-09T17:45:00+00:00,,Germany content...
https://www.dr.dk/nyheder/udland/alaska,Kom med DR til Alaska,,,,2025-03-09T16:30:00+00:00,,Alaska content...
https://www.dr.dk/nyheder/udland/flygtninge,,,Magtskifte i Syrien,,2025-03-09T15:45:00+00:00,,Flygtninge content...
